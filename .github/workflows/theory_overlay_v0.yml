#!/usr/bin/env python3
"""
theory_adapter_v0.py

Deterministic, fail-closed generator for theory_overlay_v0.json (+ optional markdown).

Intended usage:
  python scripts/theory_adapter_v0.py --pack theory_pack_v0 --out artifacts

This is CI-neutral by default: it does NOT edit the core PULSE status.json unless explicitly requested.

Design notes:
- No timestamps by default.
- All floats are rounded to a fixed precision.
- Inputs are referenced by sha256 digests.
"""

from __future__ import annotations

import argparse
import hashlib
import json
import math
import os
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple

import yaml


REQUIRED_FILES = [
    "docs/GLOSSARY_v0.md",
    "docs/STATUS_CONTRACT_v0.md",
    "docs/STATE_v0.md",
    "docs/AMBIGUITY_REGISTER_v0.md",
    "docs/CHANGELOG.md",
    "specs/AXIOMS_v0.yml",
    "specs/OP_PROTOCOLS_v0.yml",
    "specs/FIELDLAW_v0.yml",
    "specs/PREDICTIONS_v0.jsonl",
    "manifest/MANIFEST_v0.json",
]

ROUND_NDIGITS = 12


def sha256_bytes(b: bytes) -> str:
    h = hashlib.sha256()
    h.update(b)
    return h.hexdigest()


def sha256_file(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def stable_json(obj: Any) -> str:
    return json.dumps(obj, sort_keys=True, indent=2, ensure_ascii=False)


def round_floats(obj: Any, ndigits: int = ROUND_NDIGITS) -> Any:
    if isinstance(obj, float):
        v = round(obj, ndigits)
        if v == 0.0:
            v = 0.0
        return v
    if isinstance(obj, dict):
        return {k: round_floats(v, ndigits) for k, v in obj.items()}
    if isinstance(obj, list):
        return [round_floats(v, ndigits) for v in obj]
    return obj


def gate(status: str, reason: str = "") -> Dict[str, Any]:
    assert status in ("PASS", "FAIL", "MISSING")
    out = {"status": status}
    if reason:
        out["reason"] = reason
    return out


def read_jsonl(path: str) -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            items.append(json.loads(line))
    return items


@dataclass(frozen=True)
class Params:
    c: float
    mu: float
    p: float
    eta: float
    chi: float
    l0: float
    R_max: float


def compute_lambda(c: float, mu: float, p: float, r: float) -> float:
    if r <= 0:
        raise ValueError("r must be > 0")
    if abs(p) < 1e-15:
        return math.exp(-mu / (c * c * r))
    base = 1.0 - (p * mu) / (c * c * r)
    if base < 0:
        raise ValueError(f"lambda domain error: base<0 at r={r} (base={base})")
    return base ** (1.0 / p)


def compute_s(lam: float, p: float) -> float:
    if abs(p) < 1e-15:
        return 1.0
    return lam ** (-p)


def compute_g(mu: float, r: float) -> float:
    return mu / (r * r)


def compute_tidal(mu: float, r: float, s: float) -> float:
    # g = mu/r^2, dg/dr = -2mu/r^3. dg/dl = (dg/dr)/s
    return abs((-2.0 * mu / (r ** 3)) / s)


def integrate_trapz(xs: List[float], ys: List[float]) -> float:
    if len(xs) != len(ys):
        raise ValueError("xs/ys mismatch")
    if len(xs) < 2:
        return 0.0
    acc = 0.0
    for i in range(len(xs) - 1):
        dx = xs[i + 1] - xs[i]
        acc += 0.5 * dx * (ys[i] + ys[i + 1])
    return acc


def compute_K_u(params: Params, r: float) -> Tuple[float, float]:
    """
    Compute K(r) = ∫_{r}^{∞} alpha0 dl (using R_max as ∞ proxy), and u(r).
    In v0, dl = s(r) dr.
    """
    if r >= params.R_max:
        return 0.0, 0.0

    n = 2000
    xs = [r + (params.R_max - r) * i / n for i in range(n + 1)]
    alphas: List[float] = []
    dls: List[float] = []

    for rr in xs:
        lam = compute_lambda(params.c, params.mu, params.p, rr)
        ss = compute_s(lam, params.p)
        T = compute_tidal(params.mu, rr, ss)
        alpha0 = params.eta * (params.l0 / (params.c * params.c)) * T
        alphas.append(alpha0)
        dls.append(ss)  # dl/dr

    integrand = [a * dl_dr for a, dl_dr in zip(alphas, dls)]
    K = integrate_trapz(xs, integrand)

    if abs(params.chi) < 1e-15:
        u = K
    else:
        u = (math.exp(params.chi * K) - 1.0) / params.chi

    return K, u


def load_specs(pack_path: str) -> Dict[str, Any]:
    with open(os.path.join(pack_path, "specs/AXIOMS_v0.yml"), "r", encoding="utf-8") as f:
        axioms = yaml.safe_load(f)
    with open(os.path.join(pack_path, "specs/OP_PROTOCOLS_v0.yml"), "r", encoding="utf-8") as f:
        protocols = yaml.safe_load(f)
    with open(os.path.join(pack_path, "specs/FIELDLAW_v0.yml"), "r", encoding="utf-8") as f:
        fieldlaw = yaml.safe_load(f)
    preds = read_jsonl(os.path.join(pack_path, "specs/PREDICTIONS_v0.jsonl"))
    return {"axioms": axioms, "protocols": protocols, "fieldlaw": fieldlaw, "preds": preds}


def compute_inputs_digest(pack_path: str) -> str:
    h = hashlib.sha256()

    for rel in sorted(REQUIRED_FILES):
        fp = os.path.join(pack_path, rel)
        if os.path.exists(fp):
            h.update(rel.encode("utf-8"))
            h.update(b"\0")
            h.update(open(fp, "rb").read())
            h.update(b"\0")

    fixture_root = os.path.join(pack_path, "fixtures")
    for root, _, files in os.walk(fixture_root):
        for fn in sorted(files):
            if fn.endswith((".yml", ".yaml", ".json")):
                rel = os.path.relpath(os.path.join(root, fn), pack_path)
                h.update(rel.encode("utf-8"))
                h.update(b"\0")
                h.update(open(os.path.join(root, fn), "rb").read())
                h.update(b"\0")

    return h.hexdigest()


def eval_gate_artifacts_present(pack_path: str) -> Dict[str, Any]:
    missing = []
    for rel in REQUIRED_FILES:
        fp = os.path.join(pack_path, rel)
        if not os.path.exists(fp):
            missing.append(rel)
    if missing:
        return gate("FAIL", f"Missing required files: {missing}")
    return gate("PASS")


def eval_gate_semantic_lock(pack_path: str) -> Dict[str, Any]:
    manifest_path = os.path.join(pack_path, "manifest/MANIFEST_v0.json")
    if not os.path.exists(manifest_path):
        return gate("MISSING", "manifest/MANIFEST_v0.json not found")

    manifest = json.load(open(manifest_path, "r", encoding="utf-8"))
    files = manifest.get("files", [])
    mismatches = []

    for item in files:
        rel = item["path"]
        expected = item["sha256"]
        fp = os.path.join(pack_path, rel)
        if not os.path.exists(fp):
            mismatches.append({"path": rel, "reason": "missing"})
            continue
        actual = sha256_file(fp)
        if actual != expected:
            mismatches.append({"path": rel, "expected": expected, "actual": actual})

    if mismatches:
        return gate(
            "FAIL",
            f"Semantic lock mismatch; update MANIFEST_v0.json (and CHANGELOG). Details: {mismatches[:5]}",
        )
    return gate("PASS")


def eval_gate_operational_defs(specs: Dict[str, Any]) -> Dict[str, Any]:
    axioms = specs["axioms"]["axioms"]
    protos = {p["id"] for p in specs["protocols"]["protocols"]}
    missing = []
    for a in axioms:
        pid = a.get("protocol_id")
        if not pid or pid not in protos:
            missing.append({"axiom": a["id"], "protocol_id": pid})
    if missing:
        return gate("FAIL", f"Axioms missing/invalid protocol_id: {missing}")
    return gate("PASS")


def eval_gate_prediction_vector(specs: Dict[str, Any]) -> Dict[str, Any]:
    pred_ids = {p["id"] for p in specs["preds"]}
    missing = []
    for a in specs["axioms"]["axioms"]:
        if a.get("kind") in ("definition",):
            continue
        pids = a.get("predictions", [])
        if not pids:
            missing.append({"axiom": a["id"], "reason": "no predictions"})
            continue
        for pid in pids:
            if pid not in pred_ids:
                missing.append({"axiom": a["id"], "reason": f"unknown prediction id {pid}"})
    if missing:
        return gate("FAIL", f"Prediction vector missing/invalid: {missing}")
    return gate("PASS")


def eval_gate_dimensional_consistency(specs: Dict[str, Any]) -> Dict[str, Any]:
    params = specs["fieldlaw"]["parameters"]
    for name in ("c", "mu", "p", "eta", "chi", "l0", "R_max"):
        if name not in params or "units" not in params[name]:
            return gate("FAIL", f"FIELDLAW missing units for parameter: {name}")
    if params["p"]["units"] != "1" or params["eta"]["units"] != "1" or params["chi"]["units"] != "1":
        return gate("FAIL", "Expected p, eta, chi to be dimensionless (units: '1').")
    return gate("PASS")


def load_cases(pack_path: str) -> List[Dict[str, Any]]:
    cases_dir = os.path.join(pack_path, "fixtures/toy_cases_v0")
    out = []
    for fn in sorted(os.listdir(cases_dir)):
        if fn.endswith(".yml") or fn.endswith(".yaml"):
            with open(os.path.join(cases_dir, fn), "r", encoding="utf-8") as f:
                out.append(yaml.safe_load(f))
    return out


def compute_case(case: Dict[str, Any]) -> Dict[str, Any]:
    p = case["params"]
    params = Params(
        c=float(p.get("c", 1.0)),
        mu=float(p["mu"]),
        p=float(p.get("p", 0.0)),
        eta=float(p.get("eta", 0.1)),
        chi=float(p.get("chi", 1.0)),
        l0=float(p.get("l0", 1.0)),
        R_max=float(p.get("R_max", 100.0)),
    )

    r_values = [float(x) for x in case["r_values"]]
    rows = []

    for r in r_values:
        lam = compute_lambda(params.c, params.mu, params.p, r)
        s = compute_s(lam, params.p)
        g = compute_g(params.mu, r)
        T = compute_tidal(params.mu, r, s)
        alpha0 = params.eta * (params.l0 / (params.c * params.c)) * T
        K, u = compute_K_u(params, r)
        p_survive = math.exp(-u)
        rows.append(
            {
                "r": r,
                "lambda": lam,
                "s": s,
                "g": g,
                "tidal": T,
                "alpha0": alpha0,
                "K": K,
                "u": u,
                "p_survive": p_survive,
            }
        )

    out = {"case_id": case["case_id"], "params": p, "rows": rows}
    return round_floats(out)


def compare_expected(pack_path: str, case_result: Dict[str, Any]) -> Tuple[bool, str]:
    exp_path = os.path.join(pack_path, "fixtures/toy_cases_v0", f"{case_result['case_id']}_expected.json")
    if not os.path.exists(exp_path):
        return False, f"Expected file missing: {os.path.relpath(exp_path, pack_path)}"

    expected = json.load(open(exp_path, "r", encoding="utf-8"))
    if expected != case_result:
        got_h = sha256_bytes(stable_json(case_result).encode("utf-8"))
        exp_h = sha256_bytes(stable_json(expected).encode("utf-8"))
        return False, f"Mismatch expected. expected_digest={exp_h} got_digest={got_h}"

    return True, "match"


def eval_gate_regression_fixtures(pack_path: str, case_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    failures = []
    for cr in case_results:
        ok, msg = compare_expected(pack_path, cr)
        if not ok:
            failures.append({"case_id": cr["case_id"], "reason": msg})
    if failures:
        return gate("FAIL", f"Regression failures: {failures}")
    return gate("PASS")


def eval_gate_identifiability_min(case_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    if len(case_results) < 2:
        return gate("FAIL", "Need at least 2 cases for identifiability heuristic.")

    digests = []
    for cr in case_results:
        digests.append(sha256_bytes(stable_json(cr["rows"]).encode("utf-8")))

    if len(set(digests)) < 2:
        return gate("FAIL", "Identifiability heuristic failed: outputs not distinct across cases.")

    return gate("PASS")


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--pack", default="theory_pack_v0", help="Path to theory_pack_v0 directory")
    ap.add_argument("--out", default="artifacts", help="Output directory for overlay artifacts")
    ap.add_argument("--write-status-json", action="store_true", help="(Not used in v0) promotion placeholder")
    args = ap.parse_args()

    pack_path = args.pack
    out_dir = args.out
    os.makedirs(out_dir, exist_ok=True)

    # NOTE: v0 assumes theory_pack_v0 exists; we will add it in subsequent commits.
    specs = load_specs(pack_path)
    inputs_digest = compute_inputs_digest(pack_path)

    gates: Dict[str, Any] = {}
    gates["theory_artifacts_present_ok"] = eval_gate_artifacts_present(pack_path)

    if gates["theory_artifacts_present_ok"]["status"] != "PASS":
        for name in [
            "theory_semantic_lock_ok",
            "theory_operational_defs_ok",
            "theory_dimensional_consistency_ok",
            "theory_prediction_vector_present_ok",
            "theory_regression_fixtures_ok",
            "theory_identifiability_min_ok",
        ]:
            gates[name] = gate("MISSING", "Blocked by missing required artifacts.")
        cases_out: List[Dict[str, Any]] = []
    else:
        gates["theory_semantic_lock_ok"] = eval_gate_semantic_lock(pack_path)
        gates["theory_operational_defs_ok"] = eval_gate_operational_defs(specs)
        gates["theory_dimensional_consistency_ok"] = eval_gate_dimensional_consistency(specs)
        gates["theory_prediction_vector_present_ok"] = eval_gate_prediction_vector(specs)

        cases = load_cases(pack_path)
        cases_out = [compute_case(c) for c in cases]

        gates["theory_regression_fixtures_ok"] = eval_gate_regression_fixtures(pack_path, cases_out)
        gates["theory_identifiability_min_ok"] = eval_gate_identifiability_min(cases_out)

    overlay = {
        "schema": "theory_overlay_v0",
        "inputs_digest": inputs_digest,
        "gates_shadow": gates,
        "cases": cases_out,
        "evidence": {"pack_path": pack_path, "required_files": REQUIRED_FILES},
        "notes": [
            "v0 overlay: CI-neutral by default (does not modify PULSE status.json).",
            "No timestamps included to preserve determinism.",
        ],
    }

    overlay = round_floats(overlay)

    out_json = os.path.join(out_dir, "theory_overlay_v0.json")
    with open(out_json, "w", encoding="utf-8") as f:
        f.write(stable_json(overlay))
        f.write("\n")

    out_md = os.path.join(out_dir, "theory_overlay_v0.md")
    with open(out_md, "w", encoding="utf-8") as f:
        f.write("# theory_overlay_v0\n\n")
        f.write(f"- inputs_digest: `{inputs_digest}`\n\n")
        f.write("## Gates (shadow)\n\n")
        for k in sorted(gates.keys()):
            st = gates[k]["status"]
            reason = gates[k].get("reason", "")
            f.write(f"- **{k}**: `{st}`")
            if reason:
                f.write(f" — {reason}")
            f.write("\n")
        f.write("\n## Cases\n\n")
        for cr in cases_out:
            f.write(f"### {cr['case_id']}\n\n")
            f.write("Params:\n\n```json\n")
            f.write(json.dumps(cr["params"], indent=2, sort_keys=True))
            f.write("\n```\n\n")
            f.write("Rows (rounded):\n\n```json\n")
            f.write(json.dumps(cr["rows"], indent=2, sort_keys=True))
            f.write("\n```\n\n")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
