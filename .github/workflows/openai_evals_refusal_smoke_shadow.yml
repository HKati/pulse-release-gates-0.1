name: OpenAI Evals • Refusal smoke (shadow)

on:
  push:
    branches: ["main"]
    paths:
      - "openai_evals_v0/**"
      - "scripts/check_openai_evals_refusal_smoke_result_v0_contract.py"
      - "tests/test_openai_evals_refusal_smoke_dry_run_smoke.py"
      - ".github/workflows/openai_evals_refusal_smoke_shadow.yml"
  pull_request:
    paths:
      - "openai_evals_v0/**"
      - "scripts/check_openai_evals_refusal_smoke_result_v0_contract.py"
      - "tests/test_openai_evals_refusal_smoke_dry_run_smoke.py"
      - ".github/workflows/openai_evals_refusal_smoke_shadow.yml"
  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode (dry-run has no network calls / no API key required)"
        required: true
        default: "dry-run"
        type: choice
        options:
          - "dry-run"
          - "real"
      model:
        description: "Model to use in real mode"
        required: true
        default: "gpt-4.1"
        type: string
      fail_on_false:
        description: "Fail the workflow if gate_pass=false"
        required: true
        default: "false"
        type: choice
        options:
          - "false"
          - "true"
      confirm_real:
        description: "Confirm real (paid) run. Must be 'yes' when mode=real."
        required: true
        default: "no"
        type: choice
        options:
          - "no"
          - "yes"
      max_dataset_lines:
        description: "Budget guard for real runs (max dataset lines)"
        required: true
        default: "200"
        type: string

permissions:
  contents: read

concurrency:
  group: openai-evals-refusal-smoke-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  refusal_smoke:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # pinned
        with:
          fetch-depth: 1
          persist-credentials: false

      - name: Set up Python
        uses: actions/setup-python@a309ff8b426b58ec0e2a45f0f869d46889d02405 # pinned
        with:
          python-version: "3.11"

      - name: Install minimal deps
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install pyyaml

      - name: Real-run guardrails (workflow_dispatch only)
        shell: bash
        run: |
          set -euo pipefail

          EVENT_NAME="${{ github.event_name }}"
          if [[ "$EVENT_NAME" != "workflow_dispatch" ]]; then
            echo "Not workflow_dispatch; skipping real-run guardrails."
            exit 0
          fi

          MODE="${{ github.event.inputs.mode }}"
          if [[ "$MODE" != "real" ]]; then
            echo "mode=$MODE; not real; skipping real-run guardrails."
            exit 0
          fi

          CONFIRM="${{ github.event.inputs.confirm_real }}"
          if [[ "$CONFIRM" != "yes" ]]; then
            echo "::error::mode=real requires confirm_real=yes"
            exit 2
          fi

          MAX="${{ github.event.inputs.max_dataset_lines }}"
          if ! [[ "$MAX" =~ ^[0-9]+$ ]]; then
            echo "::error::max_dataset_lines must be an integer, got '$MAX'"
            exit 2
          fi

          DATASET="openai_evals_v0/refusal_smoke.jsonl"
          if [[ ! -f "$DATASET" ]]; then
            echo "::error::dataset missing: $DATASET"
            exit 2
          fi

          LINES="$(wc -l < "$DATASET" | tr -d ' ')"
          if ! [[ "$LINES" =~ ^[0-9]+$ ]]; then
            echo "::error::unable to count dataset lines for $DATASET"
            exit 2
          fi

          if (( LINES > MAX )); then
            echo "::error::dataset too large for real run: $LINES lines > max_dataset_lines=$MAX"
            exit 2
          fi

          echo "Real-run guardrails OK: dataset lines=$LINES <= max_dataset_lines=$MAX"

      - name: Prepare status.json patch target
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p PULSE_safe_pack_v0/artifacts

          EVENT_NAME="${{ github.event_name }}"
          MODE="dry-run"
          CONFIRM_REAL="no"
          if [[ "$EVENT_NAME" == "workflow_dispatch" ]]; then
            MODE="${{ github.event.inputs.mode }}"
            CONFIRM_REAL="${{ github.event.inputs.confirm_real }}"
          fi

          # Fail early before doing anything heavier if real is not explicitly confirmed
          if [[ "$EVENT_NAME" == "workflow_dispatch" && "$MODE" == "real" && "$CONFIRM_REAL" != "yes" ]]; then
            echo "::error::mode=real selected but confirm_real!=yes. Re-run with confirm_real=yes."
            exit 2
          fi

          # For manual real runs: generate a baseline PULSE status via safe pack.
          if [[ "$EVENT_NAME" == "workflow_dispatch" && "$MODE" == "real" ]]; then
            python PULSE_safe_pack_v0/tools/run_all.py
            test -f PULSE_safe_pack_v0/artifacts/status.json
          else
            # For push/PR/dry-run: keep it light and deterministic.
            # Ensure the file exists so the runner can patch it.
            if [[ ! -f PULSE_safe_pack_v0/artifacts/status.json ]]; then
              python -c 'import json; from pathlib import Path; p=Path("PULSE_safe_pack_v0/artifacts/status.json"); p.parent.mkdir(parents=True, exist_ok=True); p.write_text(json.dumps({"metrics": {}, "gates": {}}, indent=2) + "\n", encoding="utf-8")'
            fi
          fi

      - name: Run refusal smoke (OpenAI Evals v0)
        shell: bash
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_ORGANIZATION: ${{ secrets.OPENAI_ORGANIZATION }}
          OPENAI_PROJECT: ${{ secrets.OPENAI_PROJECT }}
        run: |
          set -euo pipefail

          EVENT_NAME="${{ github.event_name }}"
          # PR/push runs are dry-run by design (secrets are not exposed).
          if [[ "$EVENT_NAME" != "workflow_dispatch" ]]; then
            echo "::notice::event=$EVENT_NAME -> running in dry-run by policy (secrets are not exposed on PR/push)."
          fi

          # Defaults for push/PR: always dry-run, never fail the workflow on gate_pass=false
          MODE="dry-run"
          MODEL="gpt-4.1"
          FAIL_ON_FALSE="false"
          CONFIRM_REAL="no"
          MAX_DATASET_LINES="200"

          # For workflow_dispatch: use inputs
          if [[ "$EVENT_NAME" == "workflow_dispatch" ]]; then
            MODE="${{ github.event.inputs.mode }}"
            MODEL="${{ github.event.inputs.model }}"
            FAIL_ON_FALSE="${{ github.event.inputs.fail_on_false }}"
            CONFIRM_REAL="${{ github.event.inputs.confirm_real }}"
            MAX_DATASET_LINES="${{ github.event.inputs.max_dataset_lines }}"
          fi

          args=( python openai_evals_v0/run_refusal_smoke_to_pulse.py
                 --model "$MODEL"
                 --dataset openai_evals_v0/refusal_smoke.jsonl
                 --out openai_evals_v0/refusal_smoke_result.json
                 --status-json PULSE_safe_pack_v0/artifacts/status.json )

          if [[ "$MODE" == "dry-run" ]]; then
            args+=( --dry-run )
          else
            # Require explicit confirmation for real runs
            if [[ "$CONFIRM_REAL" != "yes" ]]; then
              echo "::error::mode=real selected but confirm_real!=yes. Re-run with confirm_real=yes."
              exit 2
            fi

            # Validate budget input is an integer (clean error message)
            if ! [[ "$MAX_DATASET_LINES" =~ ^[0-9]+$ ]]; then
              echo "::error::max_dataset_lines must be an integer, got: '$MAX_DATASET_LINES'"
              exit 2
            fi

            # Pass runner guardrails through
            args+=( --confirm-real --max-dataset-lines "$MAX_DATASET_LINES" )

            if [[ -z "${OPENAI_API_KEY:-}" ]]; then
              echo "::error::mode=real selected but OPENAI_API_KEY secret is missing."
              exit 2
            fi
          fi

          if [[ "$FAIL_ON_FALSE" == "true" ]]; then
            args+=( --fail-on-false )
          fi

          echo "Running: ${args[*]}"
          "${args[@]}"

      - name: Contract check (refusal_smoke_result.json)
        shell: bash
        run: |
          set -euo pipefail
          python scripts/check_openai_evals_refusal_smoke_result_v0_contract.py \
            --in openai_evals_v0/refusal_smoke_result.json

      - name: Status patch sanity check (metrics + gate mirror)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import sys
          from pathlib import Path

          s = Path("PULSE_safe_pack_v0/artifacts/status.json")
          if not s.exists():
              print("::warning::status.json missing (nothing to sanity-check)")
              raise SystemExit(0)

          try:
              d = json.loads(s.read_text(encoding="utf-8"))
          except Exception as e:
              print(f"::warning::status.json is not valid JSON: {e}")
              raise SystemExit(0)

          metrics = d.get("metrics")
          gates = d.get("gates")

          if not isinstance(metrics, dict):
              print(f"::warning::status.json metrics is not a dict (type={type(metrics).__name__})")
              metrics = {}

          if not isinstance(gates, dict):
              print(f"::warning::status.json gates is not a dict (type={type(gates).__name__})")
              gates = {}

          req_metrics = [
              "openai_evals_refusal_smoke_total",
              "openai_evals_refusal_smoke_passed",
              "openai_evals_refusal_smoke_failed",
              "openai_evals_refusal_smoke_errored",
              "openai_evals_refusal_smoke_fail_rate",
          ]

          missing = [k for k in req_metrics if k not in metrics]
          if missing:
              print("::warning::missing metrics in status.json: " + ", ".join(missing))

          gate_key = "openai_evals_refusal_smoke_pass"
          if gate_key not in gates:
              print(f"::warning::missing gate {gate_key} in status.json gates")
          else:
              gp = gates.get(gate_key)
              if d.get(gate_key) != gp:
                  print(f"::warning::top-level mirror {gate_key} mismatch vs status.gates")
          PY

      - name: Gate monitor (warn on false; optionally fail on dispatch)
        if: ${{ always() }}
        shell: bash
        run: |
          set -euo pipefail

          EVENT_NAME="${{ github.event_name }}"
          FAIL_ON_FALSE="false"
          if [[ "$EVENT_NAME" == "workflow_dispatch" ]]; then
            FAIL_ON_FALSE="${{ github.event.inputs.fail_on_false }}"
          fi

          export EVENT_NAME FAIL_ON_FALSE

          python -c $'import json, os, sys\nfrom pathlib import Path\np=Path("openai_evals_v0/refusal_smoke_result.json")\nif not p.exists():\n  print("::warning::missing openai_evals_v0/refusal_smoke_result.json")\n  sys.exit(0)\ntry:\n  d=json.loads(p.read_text(encoding="utf-8"))\nexcept Exception as e:\n  print(f"::warning::unable to parse refusal_smoke_result.json: {e}")\n  sys.exit(0)\n\ngp=d.get("gate_pass")\nst=d.get("status")\nif gp is not True:\n  print(f"::warning::openai_evals_refusal_smoke_pass is not passing (gate_pass={gp}, status={st})")\n\nevent=os.getenv("EVENT_NAME","")\nfof=os.getenv("FAIL_ON_FALSE","false").lower()=="true"\nif event=="workflow_dispatch" and fof and gp is not True:\n  print("::error::fail_on_false=true and gate_pass!=true")\n  sys.exit(2)\n'

      - name: Run dry-run smoke test script
        shell: bash
        run: |
          set -euo pipefail
          EVENT_NAME="${{ github.event_name }}"
          MODE="dry-run"
          if [[ "$EVENT_NAME" == "workflow_dispatch" ]]; then
            MODE="${{ github.event.inputs.mode }}"
          fi

          if [[ "$MODE" == "dry-run" ]]; then
            python tests/test_openai_evals_refusal_smoke_dry_run_smoke.py
          else
            echo "Skipping dry-run smoke test script (mode=real)."
          fi

      - name: Workflow summary
        if: always()
        shell: bash
        env:
          EVENT_NAME: ${{ github.event_name }}
          INPUT_MODE: ${{ github.event.inputs.mode }}
          INPUT_CONFIRM_REAL: ${{ github.event.inputs.confirm_real }}
          INPUT_MAX_DATASET_LINES: ${{ github.event.inputs.max_dataset_lines }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          from pathlib import Path

          summary_path = Path(os.environ["GITHUB_STEP_SUMMARY"])
          p = Path("openai_evals_v0/refusal_smoke_result.json")

          lines = []
          lines.append("## OpenAI Evals • Refusal smoke (shadow)")
          lines.append("")

          event = os.getenv("EVENT_NAME", "")
          input_mode = os.getenv("INPUT_MODE", "")
          input_confirm = os.getenv("INPUT_CONFIRM_REAL", "")
          input_max = os.getenv("INPUT_MAX_DATASET_LINES", "")

          if event and event != "workflow_dispatch":
              lines.append(f"- policy: event `{event}` forces `dry-run` (secrets are not exposed on PR/push)")
          elif event == "workflow_dispatch":
              # Inputs only exist for workflow_dispatch; still useful for audit/triage
              if input_mode:
                  lines.append(
                      f"- inputs: mode=`{input_mode}`  confirm_real=`{input_confirm or 'n/a'}`  "
                      f"max_dataset_lines=`{input_max or 'n/a'}`"
                  )

          # Run metadata (nice-to-have)
          run_no = os.getenv("GITHUB_RUN_NUMBER", "")
          sha = (os.getenv("GITHUB_SHA", "") or "")[:7]
          ref = os.getenv("GITHUB_REF_NAME", "")
          if run_no or sha or ref:
              lines.append(f"- run: `{run_no}`  sha: `{sha}`  ref: `{ref}`")

          if not p.exists():
              lines.append("")
              lines.append("_No result produced._")
              summary_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
              raise SystemExit(0)

          # Best-effort JSON parse: never fail the job because the summary can't parse.
          try:
              raw = p.read_text(encoding="utf-8")
              d = json.loads(raw)
              if not isinstance(d, dict):
                  raise TypeError(f"top-level JSON is {type(d).__name__}, expected object")
          except Exception as e:
              # GitHub annotation + summary note, then exit 0.
              print(f"::warning::unable to parse {p.as_posix()}: {e}")
              lines.append("")
              lines.append(f"⚠️ Unable to parse `{p.as_posix()}`.")
              lines.append("")
              lines.append(f"- error: `{e}`")
              summary_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
              raise SystemExit(0)

          rc = d.get("result_counts")
          if not isinstance(rc, dict):
              rc = {}

          # Canonical fields from the artifact (no recomputation here)
          lines.append(f"- mode: `{'dry-run' if d.get('dry_run') else 'real'}`")
          lines.append(f"- model: `{d.get('model')}`")
          lines.append(f"- status: `{d.get('status')}`")
          lines.append(f"- gate_key: `{d.get('gate_key')}`")
          lines.append(f"- gate_pass: **{d.get('gate_pass')}**")

          # Dataset provenance (now part of the contract)
          ds_path = d.get("dataset")
          ds_lines = d.get("dataset_lines")
          ds_sha = d.get("dataset_sha256")
          if ds_path is not None:
              lines.append(f"- dataset: `{ds_path}`")
          if ds_lines is not None:
              lines.append(f"- dataset_lines: `{ds_lines}`")
          if isinstance(ds_sha, str) and ds_sha.strip():
              lines.append(f"- dataset_sha256: `{ds_sha[:12]}`")

          lines.append("")
          lines.append("### Result counts")
          lines.append("")
          lines.append(f"- total: `{rc.get('total')}`")
          lines.append(f"- passed: `{rc.get('passed')}`")
          lines.append(f"- failed: `{rc.get('failed')}`")
          lines.append(f"- errored: `{rc.get('errored')}`")
          lines.append(f"- fail_rate: `{d.get('fail_rate')}`")

          if d.get("report_url"):
              lines.append(f"- report_url: `{d.get('report_url')}`")

          summary_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
          PY

      - name: Upload artifacts
        if: ${{ always() }}
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # pinned
        with:
          name: openai-evals-refusal-smoke-shadow
          if-no-files-found: warn
          path: |
            openai_evals_v0/refusal_smoke_result.json
            PULSE_safe_pack_v0/artifacts/status.json
            openai_evals_v0/refusal_smoke.jsonl
