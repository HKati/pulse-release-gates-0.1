name: OpenAI Evals • Refusal smoke (shadow)

on:
  push:
    branches: ["main"]
    paths:
      - "openai_evals_v0/**"
      - "scripts/check_openai_evals_refusal_smoke_result_v0_contract.py"
      - "tests/test_openai_evals_refusal_smoke_dry_run_smoke.py"
      - ".github/workflows/openai_evals_refusal_smoke_shadow.yml"
  pull_request:
    paths:
      - "openai_evals_v0/**"
      - "scripts/check_openai_evals_refusal_smoke_result_v0_contract.py"
      - "tests/test_openai_evals_refusal_smoke_dry_run_smoke.py"
      - ".github/workflows/openai_evals_refusal_smoke_shadow.yml"
  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode (dry-run has no network calls / no API key required)"
        required: true
        default: "dry-run"
        type: choice
        options:
          - "dry-run"
          - "real"
      model:
        description: "Model to use in real mode"
        required: true
        default: "gpt-4.1"
        type: string
      fail_on_false:
        description: "Fail the workflow if gate_pass=false"
        required: true
        default: "false"
        type: choice
        options:
          - "false"
          - "true"

permissions:
  contents: read

concurrency:
  group: openai-evals-refusal-smoke-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  refusal_smoke:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # pinned
        with:
          fetch-depth: 1
          persist-credentials: false

      - name: Set up Python
        uses: actions/setup-python@a309ff8b426b58ec0e2a45f0f869d46889d02405 # pinned
        with:
          python-version: "3.11"

      - name: Install minimal deps
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install pyyaml

      - name: Prepare status.json patch target
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p PULSE_safe_pack_v0/artifacts

          EVENT_NAME="${{ github.event_name }}"
          MODE="dry-run"
          if [[ "$EVENT_NAME" == "workflow_dispatch" ]]; then
            MODE="${{ github.event.inputs.mode }}"
          fi

          # For manual real runs: generate a baseline PULSE status via safe pack.
          if [[ "$EVENT_NAME" == "workflow_dispatch" && "$MODE" == "real" ]]; then
            python PULSE_safe_pack_v0/tools/run_all.py
            test -f PULSE_safe_pack_v0/artifacts/status.json
          else
            # For push/PR/dry-run: keep it light and deterministic.
            # Ensure the file exists so the runner can patch it.
            if [[ ! -f PULSE_safe_pack_v0/artifacts/status.json ]]; then
              python -c 'import json; from pathlib import Path; p=Path("PULSE_safe_pack_v0/artifacts/status.json"); p.parent.mkdir(parents=True, exist_ok=True); p.write_text(json.dumps({"metrics": {}, "gates": {}}, indent=2) + "\n", encoding="utf-8")'
            fi
          fi


      - name: Run refusal smoke (OpenAI Evals v0)
        shell: bash
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_ORGANIZATION: ${{ secrets.OPENAI_ORGANIZATION }}
          OPENAI_PROJECT: ${{ secrets.OPENAI_PROJECT }}
        run: |
          set -euo pipefail

          EVENT_NAME="${{ github.event_name }}"

          # Defaults for push/PR: always dry-run, never fail the workflow on gate_pass=false
          MODE="dry-run"
          MODEL="gpt-4.1"
          FAIL_ON_FALSE="false"

          # For workflow_dispatch: use inputs
          if [[ "$EVENT_NAME" == "workflow_dispatch" ]]; then
            MODE="${{ github.event.inputs.mode }}"
            MODEL="${{ github.event.inputs.model }}"
            FAIL_ON_FALSE="${{ github.event.inputs.fail_on_false }}"
          fi

          args=( python openai_evals_v0/run_refusal_smoke_to_pulse.py
                 --model "$MODEL"
                 --dataset openai_evals_v0/refusal_smoke.jsonl
                 --out openai_evals_v0/refusal_smoke_result.json
                 --status-json PULSE_safe_pack_v0/artifacts/status.json )

          if [[ "$MODE" == "dry-run" ]]; then
            args+=( --dry-run )
          else
            if [[ -z "${OPENAI_API_KEY:-}" ]]; then
              echo "::error::mode=real selected but OPENAI_API_KEY secret is missing."
              exit 2
            fi
          fi

          if [[ "$FAIL_ON_FALSE" == "true" ]]; then
            args+=( --fail-on-false )
          fi

          echo "Running: ${args[*]}"
          "${args[@]}"

      - name: Contract check (refusal_smoke_result.json)
        shell: bash
        run: |
          set -euo pipefail
          python scripts/check_openai_evals_refusal_smoke_result_v0_contract.py \
            --in openai_evals_v0/refusal_smoke_result.json
      
      - name: Status patch sanity check (metrics + gate mirror)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          python -c $'import json,sys\nfrom pathlib import Path\ns=Path("PULSE_safe_pack_v0/artifacts/status.json")\nif not s.exists():\n  print("::warning::status.json missing (nothing to sanity-check)")\n  sys.exit(0)\nd=json.loads(s.read_text(encoding="utf-8"))\nmetrics=d.get("metrics") or {}\ngates=d.get("gates") or {}\nreq_metrics=[\n  "openai_evals_refusal_smoke_total",\n  "openai_evals_refusal_smoke_passed",\n  "openai_evals_refusal_smoke_failed",\n  "openai_evals_refusal_smoke_errored",\n  "openai_evals_refusal_smoke_fail_rate",\n]\nmissing=[k for k in req_metrics if k not in metrics]\nif missing:\n  print("::warning::missing metrics in status.json: "+", ".join(missing))\nif "openai_evals_refusal_smoke_pass" not in gates:\n  print("::warning::missing gate openai_evals_refusal_smoke_pass in status.json gates")\nelse:\n  gp=gates.get("openai_evals_refusal_smoke_pass")\n  if d.get("openai_evals_refusal_smoke_pass") != gp:\n    print("::warning::top-level mirror openai_evals_refusal_smoke_pass mismatch vs status.gates")\n'

      - name: Gate monitor (warn on false; optionally fail on dispatch)
        if: ${{ always() }}
        shell: bash
        env:
          EVENT_NAME: ${{ github.event_name }}
          FAIL_ON_FALSE: ${{ github.event.inputs.fail_on_false }}
        run: |
          set -euo pipefail

          python -c $'import json, os, sys\nfrom pathlib import Path\np=Path("openai_evals_v0/refusal_smoke_result.json")\nif not p.exists():\n  print("::warning::missing openai_evals_v0/refusal_smoke_result.json")\n  sys.exit(0)\ntry:\n  d=json.loads(p.read_text(encoding="utf-8"))\nexcept Exception as e:\n  print(f"::warning::unable to parse refusal_smoke_result.json: {e}")\n  sys.exit(0)\n\ngp=d.get("gate_pass")\nst=d.get("status")\nif gp is not True:\n  print(f"::warning::openai_evals_refusal_smoke_pass is not passing (gate_pass={gp}, status={st})")\n\nevent=os.getenv("EVENT_NAME","")\nfof=os.getenv("FAIL_ON_FALSE","false").lower()=="true"\nif event=="workflow_dispatch" and fof and gp is not True:\n  print("::error::fail_on_false=true and gate_pass!=true")\n  sys.exit(2)\n'

      - name: Run dry-run smoke test script
        if: ${{ github.event_name != 'workflow_dispatch' || github.event.inputs.mode == 'dry-run' }}
        shell: bash
        run: |
          set -euo pipefail
          python tests/test_openai_evals_refusal_smoke_dry_run_smoke.py

      - name: Workflow summary
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          from pathlib import Path

          summary_path = Path(os.environ["GITHUB_STEP_SUMMARY"])
          p = Path("openai_evals_v0/refusal_smoke_result.json")

          lines = []
          lines.append("## OpenAI Evals • Refusal smoke (shadow)")
          lines.append("")

          if not p.exists():
              lines.append("_No refusal_smoke_result.json produced._")
              summary_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
              raise SystemExit(0)

          d = json.loads(p.read_text(encoding="utf-8"))
          rc = d.get("result_counts") or {}

          lines.append(f"- mode: `{'dry-run' if d.get('dry_run') else 'real'}`")
          lines.append(f"- model: `{d.get('model')}`")
          lines.append(f"- status: `{d.get('status')}`")
          lines.append(f"- gate_key: `{d.get('gate_key')}`")
          lines.append(f"- gate_pass: **{d.get('gate_pass')}**")
          lines.append("")
          lines.append("### Result counts")
          lines.append("")
          lines.append(f"- total: `{rc.get('total')}`")
          lines.append(f"- passed: `{rc.get('passed')}`")
          lines.append(f"- failed: `{rc.get('failed')}`")
          lines.append(f"- errored: `{rc.get('errored')}`")
          lines.append(f"- fail_rate: `{d.get('fail_rate')}`")
          if d.get("report_url"):
              lines.append(f"- report_url: `{d.get('report_url')}`")

          summary_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
          PY

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # pinned
        with:
          name: openai-evals-refusal-smoke-shadow
          if-no-files-found: warn
          path: |
            openai_evals_v0/refusal_smoke_result.json
            PULSE_safe_pack_v0/artifacts/status.json
            openai_evals_v0/refusal_smoke.jsonl
