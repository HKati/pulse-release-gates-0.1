name: Publish report pages

on:
  workflow_run:
    workflows: ["PULSE CI"]
    types: [completed]

  workflow_dispatch:
    inputs:
      run_id:
        description: "PULSE CI workflow run id to publish (Actions run ID). Optional: leave empty to use latest successful run on main."
        required: false
        type: string
      cancel_pages_deployment_id:
        description: "Optional: cancel a stuck GitHub Pages deployment before deploying (deployment id OR commit SHA)."
        required: false
        type: string

permissions:
  contents: read
  actions: read
  pages: write
  id-token: write

concurrency:
  group: "github-pages"
  cancel-in-progress: false

jobs:
  deploy:
    if: >-
      ${{
        github.event_name == 'workflow_dispatch' ||
        (github.event.workflow_run.conclusion == 'success' &&
         github.event.workflow_run.event == 'push' &&
         github.event.workflow_run.head_branch == 'main')
      }}
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deploy.outputs.page_url }}

    steps:
      - name: Resolve upstream run id
        id: runid
        shell: bash
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail

          echo "event_name: ${GITHUB_EVENT_NAME}"
          echo "repo:       ${GITHUB_REPOSITORY}"

          # Read optional workflow_dispatch inputs from the event payload (works even when inputs context is absent).
          INPUT_RUN_ID="$(python3 -c 'import json,os; ev=json.load(open(os.environ["GITHUB_EVENT_PATH"])); print((ev.get("inputs") or {}).get("run_id","").strip())')"
          INPUT_CANCEL_ID="$(python3 -c 'import json,os; ev=json.load(open(os.environ["GITHUB_EVENT_PATH"])); print((ev.get("inputs") or {}).get("cancel_pages_deployment_id","").strip())')"

          if [ "${GITHUB_EVENT_NAME}" = "workflow_dispatch" ]; then
            if [ -n "${INPUT_RUN_ID}" ]; then
              RUN_ID="${INPUT_RUN_ID}"
              echo "Using provided run_id from workflow_dispatch input: ${RUN_ID}"
            else
              echo "No run_id provided; selecting latest successful PULSE CI run on main..."

              # Prefer jq (available on ubuntu-latest); keep this YAML-safe (no heredoc).
              RUN_ID="$(
                curl -fsSL \
                  -H "Authorization: Bearer ${GH_TOKEN}" \
                  -H "Accept: application/vnd.github+json" \
                  "https://api.github.com/repos/${GITHUB_REPOSITORY}/actions/workflows/pulse_ci.yml/runs?branch=main&per_page=50" \
                | jq -r '.workflow_runs[] | select(.conclusion=="success") | .id' \
                | head -n 1
              )"

              if [ -z "${RUN_ID:-}" ] || [ "${RUN_ID}" = "null" ]; then
                echo "::error::No successful PULSE CI runs found on main (last 50). Provide run_id manually."
                exit 1
              fi
            fi
          else
            # workflow_run trigger: take run id directly from the event payload.
            RUN_ID="$(python3 -c 'import json,os; ev=json.load(open(os.environ["GITHUB_EVENT_PATH"])); print(str((ev.get("workflow_run") or {}).get("id","")).strip())')"
            if [ -z "${RUN_ID:-}" ] || [ "${RUN_ID}" = "None" ]; then
              echo "::error::workflow_run payload missing workflow_run.id; cannot proceed."
              exit 1
            fi
            echo "Using workflow_run.id from event payload: ${RUN_ID}"
          fi

          echo "run_id=${RUN_ID}" >> "$GITHUB_OUTPUT"
          echo "cancel_pages_deployment_id=${INPUT_CANCEL_ID}" >> "$GITHUB_OUTPUT"
          echo "Using run_id: ${RUN_ID}"
          if [ -n "${INPUT_CANCEL_ID}" ]; then
            echo "Cancel requested for Pages deployment id/SHA: ${INPUT_CANCEL_ID}"
          fi

      # If a previous Pages deploy is wedged "in progress", cancel it before attempting a new deployment.
      # GitHub API allows pages_deployment_id to be the deployment id OR the commit SHA.
      - name: Cancel stuck Pages deployment (manual helper)
        if: ${{ github.event_name == 'workflow_dispatch' && steps.runid.outputs.cancel_pages_deployment_id != '' }}
        shell: bash
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          CANCEL_ID: ${{ steps.runid.outputs.cancel_pages_deployment_id }}
        run: |
          set -euo pipefail

          echo "Attempting to cancel GitHub Pages deployment: ${CANCEL_ID}"

          # Best-effort: show current status (non-fatal if not found).
          curl -fsSL \
            -H "Accept: application/vnd.github+json" \
            -H "Authorization: Bearer ${GH_TOKEN}" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            "https://api.github.com/repos/${GITHUB_REPOSITORY}/pages/deployments/${CANCEL_ID}" \
            || true

          http_code="$(curl -sS -o /tmp/cancel_pages.out -w "%{http_code}" -L -X POST \
            -H "Accept: application/vnd.github+json" \
            -H "Authorization: Bearer ${GH_TOKEN}" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            "https://api.github.com/repos/${GITHUB_REPOSITORY}/pages/deployments/${CANCEL_ID}/cancel")"

          if [ "${http_code}" != "204" ]; then
            echo "::error::Failed to cancel Pages deployment ${CANCEL_ID}. HTTP ${http_code}."
            echo "::error::Response:"
            sed -n '1,200p' /tmp/cancel_pages.out || true
            exit 1
          fi

          echo "OK: cancel request accepted (204)."
          echo "Waiting briefly for Pages to release the deployment lock..."
          sleep 5

      # Checkout kept (harmless); crawler assets are generated from _site below.
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Download pulse-report artifact (from upstream run)
        uses: dawidd6/action-download-artifact@ac66b43f0e6a346234dd65d4d0c8fbb31cb316e5 # v11
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          run_id: ${{ steps.runid.outputs.run_id }}
          name: pulse-report
          path: _artifact
          if_no_artifact_found: fail

      - name: Prepare Pages site
        id: prepare
        shell: bash
        run: |
          set -euo pipefail

          rm -rf _site
          mkdir -p _site

          # Disable Jekyll processing (safer for arbitrary static assets/layouts).
          touch _site/.nojekyll

          # 1) Prefer a real index.html if present (already a proper site root).
          site_index="$(python3 - <<'PY'
          import pathlib
          root = pathlib.Path("_artifact")
          candidates = []
          for p in root.rglob("index.html"):
              s = str(p).replace("\\","/")
              if "/node_modules/" in s:
                  continue
              parts = [x.lower() for x in p.parts]
              score = 0
              for kw, w in [("report", 5), ("pages", 5), ("site", 4), ("html", 2), ("docs", 1), ("artifacts", -1)]:
                  if kw in parts:
                      score += w
              score -= len(p.parts)
              candidates.append((score, p))
          candidates.sort(reverse=True)
          print(str(candidates[0][1]) if candidates else "")
          PY
          )"

          # 2) Fallback: many PULSE runs produce report_card.html (no index.html).
          report_card="$(python3 - <<'PY'
          import pathlib
          root = pathlib.Path("_artifact")
          candidates = []
          for p in root.rglob("report_card.html"):
              s = str(p).replace("\\","/")
              if "/node_modules/" in s:
                  continue
              parts = [x.lower() for x in p.parts]
              score = 0
              for kw, w in [("pulse_safe_pack_v0", 6), ("artifacts", 6), ("report", 3), ("pages", 2), ("site", 1)]:
                  if kw in parts:
                      score += w
              score -= len(p.parts)
              candidates.append((score, p))
          candidates.sort(reverse=True)
          print(str(candidates[0][1]) if candidates else "")
          PY
          )"

          if [ -n "$site_index" ] && [ -f "$site_index" ]; then
            site_root="$(dirname "$site_index")"
            echo "Detected site root (index.html): $site_root"
            cp -a "$site_root"/. _site/

            echo "mode=index" >> "$GITHUB_OUTPUT"
            echo "site_root=$site_root" >> "$GITHUB_OUTPUT"

          elif [ -n "$report_card" ] && [ -f "$report_card" ]; then
            card_root="$(dirname "$report_card")"
            echo "Detected report_card.html: $report_card"
            echo "Using report_card directory as site root: $card_root"

            # Copy directory containing report_card.html to Pages root so relative assets keep working.
            cp -a "$card_root"/. _site/

            # Promote report_card.html to index.html at the Pages root.
            cp -a "$report_card" _site/index.html

            echo "mode=report_card" >> "$GITHUB_OUTPUT"
            echo "site_root=$card_root" >> "$GITHUB_OUTPUT"

          else
            echo "::warning::No index.html or report_card.html found in downloaded artifact; publishing raw files with a minimal landing page."
            cp -a _artifact/. _site/

            cat > _site/index.html <<'HTML'
          <!doctype html>
          <html lang="en">
            <meta charset="utf-8" />
            <meta name="viewport" content="width=device-width, initial-scale=1" />
            <title>PULSE report artifact</title>
            <body>
              <h1>PULSE report artifact</h1>
              <p>This Pages site was generated from the <code>pulse-report</code> workflow artifact.</p>
              <p>If you expected an HTML report, ensure the upstream workflow produces a <code>report_card.html</code> or <code>index.html</code>.</p>
            </body>
          </html>
          HTML

            echo "mode=raw" >> "$GITHUB_OUTPUT"
            echo "site_root=_artifact" >> "$GITHUB_OUTPUT"
          fi

          # Optional: also surface top-level extras if present in the artifact bundle.
          if [ -d "_artifact/badges" ] && [ ! -d "_site/badges" ]; then
            cp -a "_artifact/badges" "_site/"
          fi
          if [ -d "_artifact/reports" ] && [ ! -d "_site/reports" ]; then
            cp -a "_artifact/reports" "_site/"
          fi

          # --- Legacy report card URLs (must exist in deployed _site/) ---

          # Ensure /report_card.html exists at the published site root.
          # If upstream artifact already provides it, keep it.
          # Otherwise copy index.html (so legacy /report_card.html won't 404).
          if [ ! -f "_site/report_card.html" ] && [ -f "_site/index.html" ]; then
            cp -a "_site/index.html" "_site/report_card.html"
          fi

          # Ensure legacy /report_card.htm exists and points to /report_card.html.
          # Prefer the repo-maintained file if present; otherwise generate it.
          if [ -f "docs/report_card.htm" ]; then
            cp -f "docs/report_card.htm" "_site/report_card.htm"
          else
            python3 - <<'PY'
          from pathlib import Path
          Path("_site/report_card.htm").write_text(
              "<!doctype html>\n"
              "<html lang=\"en\">\n"
              "  <head>\n"
              "    <meta charset=\"utf-8\" />\n"
              "    <meta name=\"robots\" content=\"noindex,follow\" />\n"
              "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n"
              "    <link rel=\"canonical\" href=\"report_card.html\" />\n"
              "    <meta http-equiv=\"refresh\" content=\"0; url=report_card.html\" />\n"
              "    <title>Redirecting…</title>\n"
              "  </head>\n"
              "  <body>\n"
              "    <p>Redirecting to <a href=\"report_card.html\">report_card.html</a>…</p>\n"
              "  </body>\n"
              "</html>\n",
              encoding="utf-8",
          )
          print("OK: wrote _site/report_card.htm")
          PY
          fi

          # --- Paradox Core v0 (static Pages surface; Pages must not compute semantics) ---

          CASE_DIR="docs/examples/transitions_case_study_v0"
          test -d "$CASE_DIR" || { echo "::error::Missing case study dir: $CASE_DIR"; exit 1; }

          # Prefer drift/transitions inputs from the downloaded pulse-report artifact if present.
          # Deterministic selection: score by path heuristics + drift file count, tie-break by path.
          ARTIFACT_TRANSITIONS_DIR="$(python3 - <<'PY'
          import fnmatch
          import pathlib

          root = pathlib.Path("_artifact")
          cand_counts = {}

          for p in root.rglob("*"):
            if not p.is_file():
              continue
            if fnmatch.fnmatch(p.name, "pulse_*_drift_v0*"):
              d = p.parent
              key = str(d).replace("\\", "/")
              cand_counts[key] = cand_counts.get(key, 0) + 1

          scored = []
          for d, count in cand_counts.items():
            parts = [x.lower() for x in pathlib.Path(d).parts]
            score = 0

            # Prefer places where run artifacts usually live.
            if "pulse_safe_pack_v0" in parts:
              score += 6
            if "artifacts" in parts:
              score += 6
            if "reports" in parts:
              score += 2
            if any("transitions" in x for x in parts):
              score += 3
            if "logs" in parts:
              score += 1

            # Avoid test/fixture-like paths if they ever appear in artifacts.
            if "tests" in parts:
              score -= 10
            if "fixtures" in parts:
              score -= 10

            # More drift files -> better (cap keeps scoring stable).
            score += min(5, int(count))

            # Prefer shallower paths.
            score -= len(parts)

            scored.append((score, d))

          scored.sort(key=lambda t: (-t[0], t[1]))
          print(scored[0][1] if scored else "")
          PY
          )"

          TRANSITIONS_DIR=""
          PARADOX_SOURCE="case_study"

          # NEW: require a complete drift file set before selecting artifact dir (avoid partial/old artifacts).
          has_required_drift_files () {
            local d="$1"
            test -f "$d/pulse_gate_drift_v0.csv" && \
            test -f "$d/pulse_metric_drift_v0.csv" && \
            test -f "$d/pulse_overlay_drift_v0.json"
          }

          if [ -n "${ARTIFACT_TRANSITIONS_DIR:-}" ] \
            && [ -d "${ARTIFACT_TRANSITIONS_DIR:-}" ] \
            && has_required_drift_files "${ARTIFACT_TRANSITIONS_DIR:-}"; then
            TRANSITIONS_DIR="${ARTIFACT_TRANSITIONS_DIR}"
            PARADOX_SOURCE="artifact_drift"
          else
            if [ -n "${ARTIFACT_TRANSITIONS_DIR:-}" ] && [ -d "${ARTIFACT_TRANSITIONS_DIR:-}" ]; then
              echo "::warning::Artifact drift dir exists but is missing required drift files; falling back to case study."
              echo "::warning::Required: pulse_gate_drift_v0.csv, pulse_metric_drift_v0.csv, pulse_overlay_drift_v0.json"
              ls -la "${ARTIFACT_TRANSITIONS_DIR:-}" | sed 's/^/::warning::  /' || true
            fi
            TRANSITIONS_DIR="$CASE_DIR"
          fi

          echo "Paradox Core source: ${PARADOX_SOURCE} (transitions_dir=${TRANSITIONS_DIR})"
          echo "paradox_source=${PARADOX_SOURCE}" >> "$GITHUB_OUTPUT"
          echo "paradox_transitions_dir=${TRANSITIONS_DIR}" >> "$GITHUB_OUTPUT"

          rm -rf out/paradox_pages_v0
          mkdir -p out/paradox_pages_v0

          # Build paradox_field_v0 + edges from selected transitions dir
          python3 scripts/paradox_field_adapter_v0.py \
            --transitions-dir "$TRANSITIONS_DIR" \
            --out out/paradox_pages_v0/paradox_field_v0.json

          python3 scripts/check_paradox_field_v0_contract.py \
            --in out/paradox_pages_v0/paradox_field_v0.json

          python3 scripts/export_paradox_edges_v0.py \
            --in out/paradox_pages_v0/paradox_field_v0.json \
            --out out/paradox_pages_v0/paradox_edges_v0.jsonl

          python3 scripts/check_paradox_edges_v0_contract.py \
            --in out/paradox_pages_v0/paradox_edges_v0.jsonl \
            --atoms out/paradox_pages_v0/paradox_field_v0.json

          # Build the deterministic reviewer bundle (core JSON + summary MD + SVG + reviewer card HTML).
          python3 scripts/paradox_core_reviewer_bundle_v0.py \
            --field out/paradox_pages_v0/paradox_field_v0.json \
            --edges out/paradox_pages_v0/paradox_edges_v0.jsonl \
            --out-dir out/paradox_pages_v0/paradox_core_bundle_v0 \
            --k 12 \
            --metric severity

          # Publish the bundle as static assets (no compute in Pages).
          python3 scripts/pages_publish_paradox_core_bundle_v0.py \
            --bundle-dir out/paradox_pages_v0/paradox_core_bundle_v0 \
            --site-dir _site \
            --mount paradox/core/v0 \
            --write-index

          # Fail-closed: reviewer card must exist at the mount.
          test -s "_site/paradox/core/v0/paradox_core_reviewer_card_v0.html" || { echo "::error::Missing published reviewer card under _site/paradox/core/v0"; exit 1; }

          # Fail-closed: diagram JSON must exist at the mount (static publish).
          test -s "_site/paradox/core/v0/paradox_diagram_v0.json" || { echo "::error::Missing published Paradox Diagram JSON under _site/paradox/core/v0"; exit 1; }

          # Best-effort: diagram SVG is optional (some runs may legitimately skip SVG generation).
          # If present, it must be non-empty (catch broken/partial outputs).
          if [ -f "_site/paradox/core/v0/paradox_diagram_v0.svg" ]; then
            test -s "_site/paradox/core/v0/paradox_diagram_v0.svg" || { echo "::error::Paradox Diagram SVG is present but empty under _site/paradox/core/v0"; exit 1; }
          else
            echo "::warning::Paradox Diagram SVG missing under _site/paradox/core/v0 (best-effort optional)."
          fi

          # Provenance (audit-friendly; no timestamps)
          UPSTREAM_RUN_ID="${{ steps.runid.outputs.run_id }}"
          export UPSTREAM_RUN_ID PARADOX_SOURCE TRANSITIONS_DIR

          python3 - <<'PY'
          import json, os
          from pathlib import Path

          out = {
            "schema": "PULSE_paradox_pages_source_v0",
            "version": "v0",
            "upstream_run_id": os.environ.get("UPSTREAM_RUN_ID", ""),
            "source": os.environ.get("PARADOX_SOURCE", ""),
            "transitions_dir": os.environ.get("TRANSITIONS_DIR", ""),
          }

          p = Path("_site/paradox/core/v0/source_v0.json")
          p.parent.mkdir(parents=True, exist_ok=True)
          p.write_text(json.dumps(out, indent=2, sort_keys=True) + "\n", encoding="utf-8")
          print("OK: wrote", str(p))
          PY

          # --- Crawler assets (generate from _site; canonical & whitespace-proof) ---
          OWNER="${GITHUB_REPOSITORY%%/*}"
          OWNER="${OWNER,,}"
          REPO="${GITHUB_REPOSITORY#*/}"
          BASE="https://${OWNER}.github.io/${REPO}"
          BASE="${BASE%/}"
          export BASE

          # robots.txt
          python3 - <<'PY'
          from pathlib import Path
          import os

          base = os.environ["BASE"].rstrip("/")
          Path("_site/robots.txt").write_text(
              "User-agent: *\n"
              "Allow: /\n"
              f"Sitemap: {base}/sitemap.xml\n",
              encoding="utf-8",
          )
          print(f"OK: wrote _site/robots.txt (base={base})")
          PY

          # sitemap.xml from published HTML set
          python3 - <<'PY'
          import pathlib, datetime, os

          BASE = os.environ["BASE"].rstrip("/")
          root = pathlib.Path("_site")

          urls = []
          for p in sorted(root.rglob("*.html")):
              rel = p.relative_to(root).as_posix()
              if rel == "index.html":
                  urls.append(f"{BASE}/")
              elif rel.endswith("/index.html"):
                  urls.append(f"{BASE}/{rel[:-len('index.html')]}")
              else:
                  urls.append(f"{BASE}/{rel}")

          if not urls:
              raise SystemExit("::error::No HTML files found in _site; cannot generate sitemap.")

          now = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
          lines = [
              '<?xml version="1.0" encoding="UTF-8"?>',
              '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">'
          ]
          for u in urls:
              lines += ["  <url>", f"    <loc>{u}</loc>", f"    <lastmod>{now}</lastmod>", "  </url>"]
          lines.append("</urlset>")
          (root / "sitemap.xml").write_text("\n".join(lines) + "\n", encoding="utf-8")
          print(f"OK: wrote _site/sitemap.xml with {len(urls)} URLs")
          PY

          echo ""
          echo "Top-level in _site:"
          ls -la _site | sed 's/^/  /'

      - name: Surface Separation Phase overlay (stable Pages paths)
        shell: bash
        run: |
          set -euo pipefail

          # Prefer the Separation Phase overlay generated by the PULSE pack artifacts, but be robust:
          # pick the "best" separation_phase_v0.json anywhere in the downloaded artifact bundle.
          # Bonus if companion files are colocated.
          SRC_JSON="$(python3 - <<'PY'
          import pathlib

          root = pathlib.Path("_artifact")
          candidates = []
          for p in root.rglob("separation_phase_v0.json"):
              s = str(p).replace("\\", "/")
              if "/node_modules/" in s:
                  continue
              parts = [x.lower() for x in p.parts]
              score = 0
              if "pulse_safe_pack_v0" in parts:
                  score += 6
              if "artifacts" in parts:
                  score += 6
              if "overlays" in parts:
                  score += 2
              if "diagnostics" in parts:
                  score += 2
              if "reports" in parts:
                  score += 1
              if "tests" in parts or "fixtures" in parts:
                  score -= 10

              parent = p.parent
              html = parent / "separation_phase_overlay.html"
              md = parent / "separation_phase_overlay_v0.md"
              if html.exists():
                  score += 3
              if md.exists():
                  score += 2

              score -= len(parts)
              candidates.append((score, p))

          candidates.sort(key=lambda t: (-t[0], str(t[1]).replace("\\", "/")))
          print(str(candidates[0][1]) if candidates else "")
          PY
          )"

          if [ -z "${SRC_JSON:-}" ] || [ ! -f "${SRC_JSON:-}" ]; then
            echo "::notice::No separation_phase_v0.json found in _artifact; skipping separation phase surfacing."
            exit 0
          fi

          SRC_DIR="$(dirname "$SRC_JSON")"
          DEST_DIR="_site/diagnostics/separation_phase/v0"

          mkdir -p "$DEST_DIR"

          echo "Surfacing Separation Phase artifacts from: $SRC_DIR"
          echo "Into: $DEST_DIR"

          # Always copy JSON (anchor)
          cp -f "$SRC_DIR/separation_phase_v0.json" "$DEST_DIR/separation_phase_v0.json"

          # Optional companions: prefer colocated files; fall back to best-effort global search.
          SRC_MD="$SRC_DIR/separation_phase_overlay_v0.md"
          SRC_HTML="$SRC_DIR/separation_phase_overlay.html"

          if [ ! -f "$SRC_MD" ]; then
            SRC_MD="$(python3 - <<'PY'
          import pathlib
          root = pathlib.Path("_artifact")
          cands = []
          for p in root.rglob("separation_phase_overlay_v0.md"):
              s = str(p).replace("\\", "/")
              if "/node_modules/" in s:
                  continue
              parts = [x.lower() for x in p.parts]
              score = 0
              if "pulse_safe_pack_v0" in parts:
                  score += 6
              if "artifacts" in parts:
                  score += 6
              if "overlays" in parts:
                  score += 2
              if "diagnostics" in parts:
                  score += 2
              if "reports" in parts:
                  score += 1
              if "tests" in parts or "fixtures" in parts:
                  score -= 10
              score -= len(parts)
              cands.append((score, p))
          cands.sort(key=lambda t: (-t[0], str(t[1]).replace("\\", "/")))
          print(str(cands[0][1]) if cands else "")
          PY
            )"
          fi

          if [ ! -f "$SRC_HTML" ]; then
            SRC_HTML="$(python3 - <<'PY'
          import pathlib
          root = pathlib.Path("_artifact")
          cands = []
          for p in root.rglob("separation_phase_overlay.html"):
              s = str(p).replace("\\", "/")
              if "/node_modules/" in s:
                  continue
              parts = [x.lower() for x in p.parts]
              score = 0
              if "pulse_safe_pack_v0" in parts:
                  score += 6
              if "artifacts" in parts:
                  score += 6
              if "overlays" in parts:
                  score += 2
              if "diagnostics" in parts:
                  score += 2
              if "reports" in parts:
                  score += 1
              if "tests" in parts or "fixtures" in parts:
                  score -= 10
              score -= len(parts)
              cands.append((score, p))
          cands.sort(key=lambda t: (-t[0], str(t[1]).replace("\\", "/")))
          print(str(cands[0][1]) if cands else "")
          PY
            )"
          fi

          if [ -n "${SRC_MD:-}" ] && [ -f "$SRC_MD" ]; then
            cp -f "$SRC_MD" "$DEST_DIR/separation_phase_overlay_v0.md"
          else
            echo "::warning::Missing separation_phase_overlay_v0.md (no candidate found in _artifact)"
          fi

          if [ -n "${SRC_HTML:-}" ] && [ -f "$SRC_HTML" ]; then
            cp -f "$SRC_HTML" "$DEST_DIR/separation_phase_overlay.html"
          else
            echo "::warning::Missing separation_phase_overlay.html (no candidate found in _artifact)"
          fi

          # Stable short URLs at site root (convenience/backward compatibility)
          cp -f "$DEST_DIR/separation_phase_v0.json" "_site/separation_phase_v0.json"

          if [ -f "$DEST_DIR/separation_phase_overlay_v0.md" ]; then
            cp -f "$DEST_DIR/separation_phase_overlay_v0.md" "_site/separation_phase_overlay_v0.md"
          fi

          if [ -f "$DEST_DIR/separation_phase_overlay.html" ]; then
            cp -f "$DEST_DIR/separation_phase_overlay.html" "_site/separation_phase_overlay.html"
          fi

          echo "OK: Separation Phase artifacts surfaced to stable Pages paths."

      - name: Publish schemas to Pages output
        shell: bash
        run: |
          set -euo pipefail

          if [ ! -d "schemas" ]; then
            echo "::error::schemas/ directory not found in repo root; cannot publish schema URLs."
            ls -la | sed 's/^/  /' || true
            exit 1
          fi

          mkdir -p _site/schemas
          cp -r schemas/* _site/schemas/

          echo "Schemas published to _site/schemas:"
          ls -la _site/schemas | sed 's/^/  /' || true

      - name: Verify schema assets present before upload
        shell: bash
        run: |
          set -euo pipefail

          if [ ! -s "_site/schemas/gates.schema.json" ]; then
            echo "::error::Expected _site/schemas/gates.schema.json to exist and be non-empty before publish."
            ls -la _site/schemas || true
            exit 1
          fi

          if [ ! -s "_site/schemas/PULSE_paradox_field_v0.schema.json" ]; then
            echo "::error::Expected _site/schemas/PULSE_paradox_field_v0.schema.json to exist and be non-empty before publish."
            ls -la _site/schemas || true
            exit 1
          fi

          if [ ! -s "_site/schemas/separation_phase_v0.schema.json" ]; then
            echo "::error::Expected _site/schemas/separation_phase_v0.schema.json to exist and be non-empty before publish."
            ls -la _site/schemas || true
            exit 1
          fi

      - name: Generate diagnostics landing page
        shell: bash
        run: |
          set -euo pipefail

          python3 - <<'PY'
          from pathlib import Path
          import html

          site = Path("_site")
          diag = site / "diagnostics"
          diag.mkdir(parents=True, exist_ok=True)

          def exists(p: Path) -> bool:
            return p.exists() and (p.is_file() or p.is_dir())

          # NOTE: this page lives at /diagnostics/index.html,
          # so use ../ for links that live at the site root.
          items = []

          if exists(site / "report_card.html"):
            items.append(("Main report", "../report_card.html", "Primary PULSE report card surface."))

          if exists(site / "schemas"):
            items.append(("Schemas", "../schemas/", "Published JSON Schemas (contracts)."))

          if exists(site / "paradox" / "core" / "v0" / "index.html"):
            items.append(("Paradox Core v0", "../paradox/core/v0/", "Static reviewer bundle (audit surface)."))

          sp_dir = site / "diagnostics" / "separation_phase" / "v0"
          if exists(sp_dir / "separation_phase_v0.json"):
            items.append(("Separation Phase v0", "separation_phase/v0/", "CI-neutral overlay (FIELD_* classification)."))

          if exists(site / "badges"):
            items.append(("Badges", "../badges/", "Generated badges (artifact-only)."))

          if exists(site / "reports"):
            items.append(("Reports", "../reports/", "JUnit/SARIF and other report artifacts."))

          def esc(x: str) -> str:
            return html.escape(x, quote=True)

          rows = []
          for title, href, desc in items:
            rows.append(
              f'<li><a href="{esc(href)}">{esc(title)}</a>'
              f'<div style="color:#555;margin-top:4px">{esc(desc)}</div></li>'
            )

          body = f"""<!doctype html>
          <html lang="en">
            <head>
              <meta charset="utf-8" />
              <meta name="viewport" content="width=device-width, initial-scale=1" />
              <title>Diagnostics</title>
              <style>
                body {{ font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
                       margin: 24px; max-width: 980px; }}
                code {{ font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; }}
                li {{ margin: 14px 0; padding: 12px 14px; border: 1px solid #ddd; border-radius: 10px; background: #fafafa; }}
                a {{ text-decoration: none; }}
                a:hover {{ text-decoration: underline; }}
              </style>
            </head>
            <body>
              <h1>Diagnostics</h1>
              <p style="color:#555">
                Static, audit-friendly diagnostic surfaces. These pages do not change normative PULSE gating.
              </p>
              <ul style="list-style:none;padding-left:0">
                {''.join(rows) if rows else '<li>(no diagnostics surfaced in this run)</li>'}
              </ul>
            </body>
          </html>
          """

          (diag / "index.html").write_text(body + "\n", encoding="utf-8")

          # Convenience redirect at site root: /diagnostics.html -> /diagnostics/
          (site / "diagnostics.html").write_text(
            "<!doctype html><html lang=\"en\"><meta charset=\"utf-8\" />"
            "<meta http-equiv=\"refresh\" content=\"0; url=diagnostics/\" />"
            "<title>Redirecting…</title><body>"
            "<p>Redirecting to <a href=\"diagnostics/\">diagnostics/</a>…</p></body></html>\n",
            encoding="utf-8",
          )

          print("OK: wrote _site/diagnostics/index.html and _site/diagnostics.html")
          PY

      - name: Regenerate sitemap.xml from final _site
        shell: bash
        run: |
          set -euo pipefail

          OWNER="${GITHUB_REPOSITORY%%/*}"
          OWNER="${OWNER,,}"
          REPO="${GITHUB_REPOSITORY#*/}"
          BASE="https://${OWNER}.github.io/${REPO}"
          BASE="${BASE%/}"
          export BASE

          python3 - <<'PY'
          import pathlib, datetime, os

          BASE = os.environ["BASE"].rstrip("/")
          root = pathlib.Path("_site")

          urls = []
          for p in sorted(root.rglob("*.html")):
              rel = p.relative_to(root).as_posix()
              if rel == "index.html":
                  urls.append(f"{BASE}/")
              elif rel.endswith("/index.html"):
                  urls.append(f"{BASE}/{rel[:-len('index.html')]}")
              else:
                  urls.append(f"{BASE}/{rel}")

          if not urls:
              raise SystemExit("::error::No HTML files found in _site; cannot generate sitemap.")

          now = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
          lines = [
              '<?xml version="1.0" encoding="UTF-8"?>',
              '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">'
          ]
          for u in urls:
              lines += ["  <url>", f"    <loc>{u}</loc>", f"    <lastmod>{now}</lastmod>", "  </url>"]
          lines.append("</urlset>")

          (root / "sitemap.xml").write_text("\n".join(lines) + "\n", encoding="utf-8")
          print(f"OK: rewrote _site/sitemap.xml with {len(urls)} URLs")
          PY

      - name: Verify crawler assets present before upload
        shell: bash
        run: |
          set -euo pipefail

          if [ ! -s "_site/sitemap.xml" ]; then
            echo "::error::Expected _site/sitemap.xml to exist and be non-empty before publish."
            ls -la _site || true
            exit 1
          fi

          if [ ! -s "_site/robots.txt" ]; then
            echo "::error::Expected _site/robots.txt to exist and be non-empty before publish."
            ls -la _site || true
            exit 1
          fi

          if [ "$(wc -l < _site/robots.txt)" -lt 3 ]; then
            echo "::error::robots.txt must be multi-line (>=3 lines)."
            echo "--- robots.txt ---"
            cat _site/robots.txt || true
            exit 1
          fi

      - name: Workflow summary (Pages publish)
        if: always()
        shell: bash
        run: |
          set -euo pipefail

          echo "## Publish report pages" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "- upstream run_id: \`${{ steps.runid.outputs.run_id }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- publish mode: \`${{ steps.prepare.outputs.mode }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- site root used: \`${{ steps.prepare.outputs.site_root }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- paradox_source: \`${{ steps.prepare.outputs.paradox_source }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- paradox_transitions_dir: \`${{ steps.prepare.outputs.paradox_transitions_dir }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          if [ -f "_site/index.html" ]; then
            echo "- ✅ \`_site/index.html\` present" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "- ❌ \`_site/index.html\` missing" >> "$GITHUB_STEP_SUMMARY"
          fi

          if [ -f "_site/sitemap.xml" ]; then
            echo "- ✅ \`_site/sitemap.xml\` present" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "- ❌ \`_site/sitemap.xml\` missing" >> "$GITHUB_STEP_SUMMARY"
          fi

          if [ -f "_site/robots.txt" ]; then
            echo "- ✅ \`_site/robots.txt\` present" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "- ❌ \`_site/robots.txt\` missing" >> "$GITHUB_STEP_SUMMARY"
          fi

          if [ -f "_site/paradox/core/v0/paradox_core_reviewer_card_v0.html" ]; then
            echo "- ✅ Paradox Core: \`/paradox/core/v0/\` mounted" >> "$GITHUB_STEP_SUMMARY"
          fi

          if [ -f "_site/diagnostics/separation_phase/v0/separation_phase_overlay.html" ]; then
            echo "- ✅ Separation Phase: \`/diagnostics/separation_phase/v0/\` mounted" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "- ⚠️ Separation Phase: not present in _site (may be absent in this run)" >> "$GITHUB_STEP_SUMMARY"
          fi

          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### _site top-level" >> "$GITHUB_STEP_SUMMARY"
          echo '```' >> "$GITHUB_STEP_SUMMARY"
          ls -la _site >> "$GITHUB_STEP_SUMMARY" || true
          echo '```' >> "$GITHUB_STEP_SUMMARY"

      - name: Setup Pages
        uses: actions/configure-pages@983d7736d9b0ae728b81ab479565c72886d7745b # v5.0.0

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@7b1f4a764d45c48632c6b24a0339c27f5614fb0b # v4.0.0
        with:
          path: _site

      - name: Deploy to GitHub Pages
        id: deploy
        uses: actions/deploy-pages@d6db90164ac5ed86f2b6aed7e0febac5b3c0c03e # v4.0.5

      # Post-deploy: verify the *public* Pages site serves crawler-critical endpoints.
      - name: SEO smoke (post-deploy)
        if: ${{ steps.deploy.outcome == 'success' }}
        shell: bash
        run: |
          set -euo pipefail

          BASE="${{ steps.deploy.outputs.page_url }}"
          BASE="${BASE%/}"
          export BASE

          if [ -z "${BASE:-}" ]; then
            echo "::error::No page_url output from deploy step; cannot run SEO smoke."
            exit 1
          fi

          echo "Pages base: $BASE"

          fetch () {
            local url="$1"
            local out="$2"
            local i=0
            while true; do
              i=$((i+1))
              if curl -fsSL "$url" -o "$out"; then
                return 0
              fi
              if [ "$i" -ge 10 ]; then
                echo "::error::Failed to fetch $url after $i attempts."
                return 1
              fi
              echo "Retry $i/10: $url"
              sleep 3
            done
          }

          fetch_optional () {
            local url="$1"
            local out="$2"
            if curl -fsSL "$url" -o "$out"; then
              return 0
            fi
            echo "::warning::Optional resource not available (best-effort): $url"
            rm -f "$out" || true
            return 0
          }

          fetch_headers () {
            local url="$1"
            local out="$2"
            local i=0
            while true; do
              i=$((i+1))
              if curl -fsSIL -D "$out" -o /dev/null "$url"; then
                return 0
              fi
              if [ "$i" -ge 10 ]; then
                echo "::error::Failed to fetch headers for $url after $i attempts."
                return 1
              fi
              echo "Retry $i/10 (headers): $url"
              sleep 3
            done
          }

          fetch_headers_optional () {
            local url="$1"
            local out="$2"
            if curl -fsSIL -D "$out" -o /dev/null "$url"; then
              return 0
            fi
            echo "::warning::Optional headers not available (best-effort): $url"
            rm -f "$out" || true
            return 0
          }

          fetch "$BASE/robots.txt" robots.txt
          fetch "$BASE/sitemap.xml" sitemap.xml
          fetch "$BASE/report_card.html" report_card.html
          fetch "$BASE/report_card.htm" report_card.htm

          fetch "$BASE/schemas/gates.schema.json" schema_gates.schema.json
          fetch "$BASE/schemas/PULSE_paradox_field_v0.schema.json" schema_paradox_field.schema.json
          fetch "$BASE/schemas/separation_phase_v0.schema.json" schema_separation_phase.schema.json

          fetch "$BASE/paradox/core/v0/" paradox_core_index.html
          fetch "$BASE/paradox/core/v0/paradox_core_reviewer_card_v0.html" paradox_core_card.html
          fetch "$BASE/paradox/core/v0/source_v0.json" paradox_core_source.json

          fetch "$BASE/paradox/core/v0/paradox_diagram_v0.json" paradox_diagram.json
          fetch_optional "$BASE/paradox/core/v0/paradox_diagram_v0.svg" paradox_diagram.svg

          # Separation Phase overlay (best-effort: may be absent for older runs)
          fetch_optional "$BASE/separation_phase_overlay.html" separation_phase_overlay_root.html
          fetch_optional "$BASE/separation_phase_v0.json" separation_phase_v0_root.json
          fetch_optional "$BASE/separation_phase_overlay_v0.md" separation_phase_overlay_root.md

          fetch_optional "$BASE/diagnostics/separation_phase/v0/separation_phase_overlay.html" separation_phase_overlay_diag.html
          fetch_optional "$BASE/diagnostics/separation_phase/v0/separation_phase_v0.json" separation_phase_v0_diag.json
          fetch_optional "$BASE/diagnostics/separation_phase/v0/separation_phase_overlay_v0.md" separation_phase_overlay_diag.md
          fetch_optional "$BASE/diagnostics/" diagnostics_index.html
          fetch_optional "$BASE/diagnostics.html" diagnostics_redirect.html

          fetch_headers "$BASE/" headers_home.txt
          fetch_headers "$BASE/robots.txt" headers_robots.txt
          fetch_headers "$BASE/sitemap.xml" headers_sitemap.txt
          fetch_headers "$BASE/paradox/core/v0/" headers_paradox_core.txt
          fetch_headers "$BASE/paradox/core/v0/source_v0.json" headers_paradox_core_source.txt
          fetch_headers "$BASE/paradox/core/v0/paradox_diagram_v0.json" headers_paradox_diagram_json.txt
          fetch_headers_optional "$BASE/paradox/core/v0/paradox_diagram_v0.svg" headers_paradox_diagram_svg.txt

          check_noindex_header () {
            local url="$1"
            local file="$2"
            if [ ! -f "$file" ]; then
              return 0
            fi
            if grep -Eqi '^x-robots-tag:\s*.*noindex' "$file"; then
              echo "::error::X-Robots-Tag noindex detected for $url. This blocks indexing."
              echo "::error::Fix: Repo Settings -> Pages -> Search engine visibility must allow indexing."
              echo "Headers (head):"
              sed -n '1,200p' "$file"
              exit 1
            fi
          }

          check_noindex_header "$BASE/" headers_home.txt
          check_noindex_header "$BASE/robots.txt" headers_robots.txt
          check_noindex_header "$BASE/sitemap.xml" headers_sitemap.txt
          check_noindex_header "$BASE/paradox/core/v0/" headers_paradox_core.txt
          check_noindex_header "$BASE/paradox/core/v0/source_v0.json" headers_paradox_core_source.txt
          check_noindex_header "$BASE/paradox/core/v0/paradox_diagram_v0.json" headers_paradox_diagram_json.txt
          check_noindex_header "$BASE/paradox/core/v0/paradox_diagram_v0.svg" headers_paradox_diagram_svg.txt

          echo "OK: post-deploy SEO smoke passed."
