name: Publish report pages

on:
  workflow_run:
    workflows: ["PULSE CI"]
    types: [completed]

  workflow_dispatch:
    inputs:
      run_id:
        description: "PULSE CI workflow run id to publish (Actions run ID). Optional: leave empty to use latest successful run on main."
        required: false
        type: string

permissions:
  contents: read
  actions: read
  pages: write
  id-token: write

concurrency:
  group: "github-pages"
  cancel-in-progress: false

jobs:
  deploy:
    if: >-
      ${{
        github.event_name == 'workflow_dispatch' ||
        (github.event.workflow_run.conclusion == 'success' &&
         github.event.workflow_run.event == 'push' &&
         github.event.workflow_run.head_branch == 'main')
      }}
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deploy.outputs.page_url }}

    steps:
      # Checkout kept (harmless); crawler assets are generated from _site below.
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Resolve upstream run id
        id: runid
        shell: bash
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail

          echo "event_name: ${GITHUB_EVENT_NAME}"
          echo "repo:       ${GITHUB_REPOSITORY}"

          # Read optional workflow_dispatch input from the event payload (works even when inputs context is absent).
          INPUT_RUN_ID="$(python3 -c 'import json,os; ev=json.load(open(os.environ["GITHUB_EVENT_PATH"])); print((ev.get("inputs") or {}).get("run_id","").strip())')"

          if [ "${GITHUB_EVENT_NAME}" = "workflow_dispatch" ]; then
            if [ -n "${INPUT_RUN_ID}" ]; then
              RUN_ID="${INPUT_RUN_ID}"
              echo "Using provided run_id from workflow_dispatch input: ${RUN_ID}"
            else
              echo "No run_id provided; selecting latest successful PULSE CI run on main..."

              # Prefer jq (available on ubuntu-latest); keep this YAML-safe (no heredoc).
              RUN_ID="$(
                curl -fsSL \
                  -H "Authorization: Bearer ${GH_TOKEN}" \
                  -H "Accept: application/vnd.github+json" \
                  "https://api.github.com/repos/${GITHUB_REPOSITORY}/actions/workflows/pulse_ci.yml/runs?branch=main&per_page=50" \
                | jq -r '.workflow_runs[] | select(.conclusion=="success") | .id' \
                | head -n 1
              )"

              if [ -z "${RUN_ID:-}" ] || [ "${RUN_ID}" = "null" ]; then
                echo "::error::No successful PULSE CI runs found on main (last 50). Provide run_id manually."
                exit 1
              fi
            fi
          else
            # workflow_run trigger: take run id directly from the event payload.
            RUN_ID="$(python3 -c 'import json,os; ev=json.load(open(os.environ["GITHUB_EVENT_PATH"])); print(str((ev.get("workflow_run") or {}).get("id","")).strip())')"
            if [ -z "${RUN_ID:-}" ] || [ "${RUN_ID}" = "None" ]; then
              echo "::error::workflow_run payload missing workflow_run.id; cannot proceed."
              exit 1
            fi
            echo "Using workflow_run.id from event payload: ${RUN_ID}"
          fi

          echo "run_id=${RUN_ID}" >> "$GITHUB_OUTPUT"
          echo "Using run_id: ${RUN_ID}"

      - name: Download pulse-report artifact (from upstream run)
        uses: dawidd6/action-download-artifact@ac66b43f0e6a346234dd65d4d0c8fbb31cb316e5 # v11
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          run_id: ${{ steps.runid.outputs.run_id }}
          name: pulse-report
          path: _artifact
          if_no_artifact_found: fail

      - name: Prepare Pages site
        id: prepare
        shell: bash
        run: |
          set -euo pipefail

          rm -rf _site
          mkdir -p _site

          # Disable Jekyll processing (safer for arbitrary static assets/layouts).
          touch _site/.nojekyll

          # 1) Prefer a real index.html if present (already a proper site root).
          site_index="$(python3 - <<'PY'
          import pathlib
          root = pathlib.Path("_artifact")
          candidates = []
          for p in root.rglob("index.html"):
              s = str(p).replace("\\\\","/")
              if "/node_modules/" in s:
                  continue
              parts = [x.lower() for x in p.parts]
              score = 0
              for kw, w in [("report", 5), ("pages", 5), ("site", 4), ("html", 2), ("docs", 1), ("artifacts", -1)]:
                  if kw in parts:
                      score += w
              score -= len(p.parts)
              candidates.append((score, p))
          candidates.sort(reverse=True)
          print(str(candidates[0][1]) if candidates else "")
          PY
          )"

          # 2) Fallback: many PULSE runs produce report_card.html (no index.html).
          report_card="$(python3 - <<'PY'
          import pathlib
          root = pathlib.Path("_artifact")
          candidates = []
          for p in root.rglob("report_card.html"):
              s = str(p).replace("\\\\","/")
              if "/node_modules/" in s:
                  continue
              parts = [x.lower() for x in p.parts]
              score = 0
              for kw, w in [("pulse_safe_pack_v0", 6), ("artifacts", 6), ("report", 3), ("pages", 2), ("site", 1)]:
                  if kw in parts:
                      score += w
              score -= len(p.parts)
              candidates.append((score, p))
          candidates.sort(reverse=True)
          print(str(candidates[0][1]) if candidates else "")
          PY
          )"

          if [ -n "$site_index" ] && [ -f "$site_index" ]; then
            site_root="$(dirname "$site_index")"
            echo "Detected site root (index.html): $site_root"
            cp -a "$site_root"/. _site/

            echo "mode=index" >> "$GITHUB_OUTPUT"
            echo "site_root=$site_root" >> "$GITHUB_OUTPUT"

          elif [ -n "$report_card" ] && [ -f "$report_card" ]; then
            card_root="$(dirname "$report_card")"
            echo "Detected report_card.html: $report_card"
            echo "Using report_card directory as site root: $card_root"

            # Copy directory containing report_card.html to Pages root so relative assets keep working.
            cp -a "$card_root"/. _site/

            # Promote report_card.html to index.html at the Pages root.
            cp -a "$report_card" _site/index.html

            echo "mode=report_card" >> "$GITHUB_OUTPUT"
            echo "site_root=$card_root" >> "$GITHUB_OUTPUT"

          else
            echo "::warning::No index.html or report_card.html found in downloaded artifact; publishing raw files with a minimal landing page."
            cp -a _artifact/. _site/

            cat > _site/index.html <<'HTML'
          <!doctype html>
          <html lang="en">
            <meta charset="utf-8" />
            <meta name="viewport" content="width=device-width, initial-scale=1" />
            <title>PULSE report artifact</title>
            <body>
              <h1>PULSE report artifact</h1>
              <p>This Pages site was generated from the <code>pulse-report</code> workflow artifact.</p>
              <p>If you expected an HTML report, ensure the upstream workflow produces a <code>report_card.html</code> or <code>index.html</code>.</p>
            </body>
          </html>
          HTML

            echo "mode=raw" >> "$GITHUB_OUTPUT"
            echo "site_root=_artifact" >> "$GITHUB_OUTPUT"
          fi

          # Optional: also surface top-level extras if present in the artifact bundle.
          if [ -d "_artifact/badges" ] && [ ! -d "_site/badges" ]; then
            cp -a "_artifact/badges" "_site/"
          fi
          if [ -d "_artifact/reports" ] && [ ! -d "_site/reports" ]; then
            cp -a "_artifact/reports" "_site/"
          fi

          # --- Legacy report card URLs (must exist in deployed _site/) ---

          # Ensure /report_card.html exists at the published site root.
          # If upstream artifact already provides it, keep it.
          # Otherwise copy index.html (so legacy /report_card.html won't 404).
          if [ ! -f "_site/report_card.html" ] && [ -f "_site/index.html" ]; then
            cp -a "_site/index.html" "_site/report_card.html"
          fi

          # Ensure legacy /report_card.htm exists and points to /report_card.html.
          # Prefer the repo-maintained file if present; otherwise generate it.
          if [ -f "docs/report_card.htm" ]; then
            cp -f "docs/report_card.htm" "_site/report_card.htm"
          else
            python3 - <<'PY'
          from pathlib import Path
          Path("_site/report_card.htm").write_text(
              "<!doctype html>\n"
              "<html lang=\"en\">\n"
              "  <head>\n"
              "    <meta charset=\"utf-8\" />\n"
              "    <meta name=\"robots\" content=\"noindex,follow\" />\n"
              "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n"
              "    <link rel=\"canonical\" href=\"report_card.html\" />\n"
              "    <meta http-equiv=\"refresh\" content=\"0; url=report_card.html\" />\n"
              "    <title>Redirecting…</title>\n"
              "  </head>\n"
              "  <body>\n"
              "    <p>Redirecting to <a href=\"report_card.html\">report_card.html</a>…</p>\n"
              "  </body>\n"
              "</html>\n",
              encoding="utf-8",
          )
          print("OK: wrote _site/report_card.htm")
          PY
          fi

          # --- Crawler assets (generate from _site; canonical & whitespace-proof) ---
          OWNER="${GITHUB_REPOSITORY%%/*}"
          OWNER="${OWNER,,}"
          REPO="${GITHUB_REPOSITORY#*/}"
          BASE="https://${OWNER}.github.io/${REPO}"
          BASE="${BASE%/}"
          export BASE

          # robots.txt (guaranteed LF newlines, no heredoc indentation pitfalls)
          python3 - <<'PY'
          from pathlib import Path
          import os

          base = os.environ["BASE"].rstrip("/")
          Path("_site/robots.txt").write_text(
              "User-agent: *\n"
              "Allow: /\n"
              f"Sitemap: {base}/sitemap.xml\n",
              encoding="utf-8",
          )
          print(f"OK: wrote _site/robots.txt (base={base})")
          PY

          # sitemap.xml from actual published HTML set
          python3 - <<'PY'
          import pathlib, datetime, os

          BASE = os.environ["BASE"].rstrip("/")
          root = pathlib.Path("_site")

          urls = []
          for p in sorted(root.rglob("*.html")):
              rel = p.relative_to(root).as_posix()
              if rel == "index.html":
                  urls.append(f"{BASE}/")
              else:
                  urls.append(f"{BASE}/{rel}")

          if not urls:
              raise SystemExit("::error::No HTML files found in _site; cannot generate sitemap.")

          now = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
          lines = [
              '<?xml version="1.0" encoding="UTF-8"?>',
              '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">'
          ]
          for u in urls:
              lines += ["  <url>", f"    <loc>{u}</loc>", f"    <lastmod>{now}</lastmod>", "  </url>"]
          lines.append("</urlset>")
          (root / "sitemap.xml").write_text("\n".join(lines) + "\n", encoding="utf-8")
          print(f"OK: wrote _site/sitemap.xml with {len(urls)} URLs")
          PY

          echo ""
          echo "Top-level in _site:"
          ls -la _site | sed 's/^/  /'

      - name: Verify crawler assets present before upload
        shell: bash
        run: |
          set -euo pipefail

          if [ ! -s "_site/sitemap.xml" ]; then
            echo "::error::Expected _site/sitemap.xml to exist and be non-empty before publish."
            ls -la _site || true
            exit 1
          fi

          if [ ! -s "_site/robots.txt" ]; then
            echo "::error::Expected _site/robots.txt to exist and be non-empty before publish."
            ls -la _site || true
            exit 1
          fi

          # Guardrail: robots must be multiline
          if [ "$(wc -l < _site/robots.txt)" -lt 3 ]; then
            echo "::error::robots.txt must be multi-line (>=3 lines)."
            echo "--- robots.txt ---"
            cat _site/robots.txt || true
            exit 1
          fi

      - name: Workflow summary (Pages publish)
        if: always()
        shell: bash
        run: |
          set -euo pipefail

          echo "## Publish report pages" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "- upstream run_id: \`${{ steps.runid.outputs.run_id }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- publish mode: \`${{ steps.prepare.outputs.mode }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- site root used: \`${{ steps.prepare.outputs.site_root }}\`" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          if [ -f "_site/index.html" ]; then
            echo "- ✅ \`_site/index.html\` present" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "- ❌ \`_site/index.html\` missing" >> "$GITHUB_STEP_SUMMARY"
          fi

          if [ -f "_site/sitemap.xml" ]; then
            echo "- ✅ \`_site/sitemap.xml\` present" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "- ❌ \`_site/sitemap.xml\` missing" >> "$GITHUB_STEP_SUMMARY"
          fi

          if [ -f "_site/robots.txt" ]; then
            echo "- ✅ \`_site/robots.txt\` present" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "- ❌ \`_site/robots.txt\` missing" >> "$GITHUB_STEP_SUMMARY"
          fi

          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "### _site top-level" >> "$GITHUB_STEP_SUMMARY"
          echo '```' >> "$GITHUB_STEP_SUMMARY"
          ls -la _site >> "$GITHUB_STEP_SUMMARY" || true
          echo '```' >> "$GITHUB_STEP_SUMMARY"

      - name: Setup Pages
        uses: actions/configure-pages@983d7736d9b0ae728b81ab479565c72886d7745b # v5.0.0

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@7b1f4a764d45c48632c6b24a0339c27f5614fb0b # v4.0.0
        with:
          path: _site

      - name: Deploy to GitHub Pages
        id: deploy
        uses: actions/deploy-pages@d6db90164ac5ed86f2b6aed7e0febac5b3c0c03e # v4.0.5

      # Post-deploy: verify the *public* Pages site serves crawler-critical endpoints.
      - name: SEO smoke (post-deploy)
        if: ${{ steps.deploy.outcome == 'success' }}
        shell: bash
        run: |
          set -euo pipefail

          BASE="${{ steps.deploy.outputs.page_url }}"
          BASE="${BASE%/}"
          export BASE

          if [ -z "${BASE:-}" ]; then
            echo "::error::No page_url output from deploy step; cannot run SEO smoke."
            exit 1
          fi

          echo "Pages base: $BASE"

          fetch () {
            local url="$1"
            local out="$2"
            local i=0
            while true; do
              i=$((i+1))
              if curl -fsSL "$url" -o "$out"; then
                return 0
              fi
              if [ "$i" -ge 10 ]; then
                echo "::error::Failed to fetch $url after $i attempts."
                return 1
              fi
              echo "Retry $i/10: $url"
              sleep 3
            done
          }

          fetch_headers () {
            local url="$1"
            local out="$2"
            local i=0
            while true; do
              i=$((i+1))
              # -I = HEAD, -L = follow redirects, -D = dump headers
              if curl -fsSIL -D "$out" -o /dev/null "$url"; then
                return 0
              fi
              if [ "$i" -ge 10 ]; then
                echo "::error::Failed to fetch headers for $url after $i attempts."
                return 1
              fi
              echo "Retry $i/10 (headers): $url"
              sleep 3
            done
          }

          fetch "$BASE/robots.txt" robots.txt
          fetch "$BASE/sitemap.xml" sitemap.xml
          fetch "$BASE/report_card.html" report_card.html
          fetch "$BASE/report_card.htm" report_card.htm

          # Headers: detect hard indexing blocks (X-Robots-Tag: noindex)
          fetch_headers "$BASE/" headers_home.txt
          fetch_headers "$BASE/robots.txt" headers_robots.txt
          fetch_headers "$BASE/sitemap.xml" headers_sitemap.txt

          echo "homepage headers (head):"
          sed -n '1,120p' headers_home.txt

          check_noindex_header () {
            local url="$1"
            local file="$2"
            if grep -Eqi '^x-robots-tag:\s*.*noindex' "$file"; then
              echo "::error::X-Robots-Tag noindex detected for $url. This blocks indexing."
              echo "::error::Fix: Repo Settings → Pages → Search engine visibility must allow indexing."
              echo "Headers (head):"
              sed -n '1,200p' "$file"
              exit 1
            fi
          }

          check_noindex_header "$BASE/" headers_home.txt
          check_noindex_header "$BASE/robots.txt" headers_robots.txt
          check_noindex_header "$BASE/sitemap.xml" headers_sitemap.txt

          # Best-effort homepage fetch (for meta noindex detection).
          if curl -fsSL "$BASE/" -o index.html; then
            :
          else
            echo "::warning::Could not fetch homepage HTML; skipping meta robots check."
            rm -f index.html || true
          fi

          echo "robots.txt (head):"
          sed -n '1,120p' robots.txt

          echo "sitemap.xml (head):"
          sed -n '1,120p' sitemap.xml

          # Robust robots validation (avoid regex footguns)
          python3 - <<'PY'
          import os, sys
          from pathlib import Path

          BASE = os.environ.get("BASE","").rstrip("/")
          txt = Path("robots.txt").read_text(encoding="utf-8", errors="replace").splitlines()
          norm = [ln.strip() for ln in txt if ln.strip()]

          if len(norm) < 3:
            print("::error::robots.txt must contain at least 3 non-empty lines.")
            for ln in txt[:20]:
              print(ln)
            sys.exit(1)

          lower = [ln.lower() for ln in norm]
          need = [
            "user-agent: *",
            "allow: /",
            f"sitemap: {BASE}/sitemap.xml".lower(),
          ]
          missing = [x for x in need if x not in lower]
          if missing:
            print("::error::robots.txt missing required directive lines:")
            for m in missing:
              print(" - " + m)
            sys.exit(1)

          print("OK: robots.txt directives validated.")
          PY

          # Validate sitemap is well-formed XML and locs are canonical (regex-safe, fail-closed)
          python3 - <<'PY'
          import os
          import xml.etree.ElementTree as ET
          from pathlib import Path

          BASE = os.environ.get("BASE","").rstrip("/")
          home = f"{BASE}/"

          xml_text = Path("sitemap.xml").read_text(encoding="utf-8", errors="replace")

          try:
            root = ET.fromstring(xml_text)
          except Exception as e:
            raise SystemExit(f"::error::sitemap.xml is not valid XML: {e}")

          locs = [(el.text or "").strip() for el in root.findall(".//{*}loc")]
          if not locs:
            raise SystemExit("::error::sitemap.xml contains no <loc> entries.")

          # Must include homepage exactly (canonical).
          if home not in locs:
            raise SystemExit(f"::error::sitemap.xml missing homepage <loc>{home}</loc>")

          # Every loc must be under BASE/ (fail closed if any typo slips in).
          bad = [u for u in locs if not (u == home or u.startswith(home))]
          if bad:
            raise SystemExit(f"::error::sitemap.xml contains <loc> outside BASE={home}: {bad[:5]}")

          print(f"OK: sitemap.xml parsed; loc_count={len(locs)}; homepage_present=yes")
          PY

          # guardrail: accidental noindex in homepage HTML blocks indexing even if robots is open
          if [ -f index.html ]; then
            python3 - <<'PY'
          import re
          import sys
          from pathlib import Path

          p = Path("index.html")
          html = p.read_text(encoding="utf-8", errors="ignore")

          meta_tags = re.findall(r"<meta\b[^>]*>", html, flags=re.I)
          bad = []
          for tag in meta_tags:
            t = tag.lower()
            if "noindex" not in t:
              continue
            if re.search(r"\bname\s*=\s*['\"]?(robots|googlebot)['\"]?\b", t) or re.search(r"\bhttp-equiv\s*=\s*['\"]?x-robots-tag['\"]?\b", t):
              bad.append(tag.strip())

          if bad:
            print("::error::Found meta noindex directive(s) in homepage HTML (blocks indexing):")
            for tag in bad[:10]:
              print(tag)
            sys.exit(1)

          print("OK: no meta noindex found in homepage HTML.")
          PY
          fi

          echo "## SEO smoke (post-deploy)" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "- base: \`$BASE\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- ✅ robots: \`$BASE/robots.txt\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- ✅ sitemap: \`$BASE/sitemap.xml\`" >> "$GITHUB_STEP_SUMMARY"
          echo "- ✅ no X-Robots-Tag: noindex" >> "$GITHUB_STEP_SUMMARY"
          echo "- ✅ robots directives OK" >> "$GITHUB_STEP_SUMMARY"
          echo "- ✅ sitemap XML parse OK (canonical, fail-closed)" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          echo "OK: post-deploy SEO smoke passed."

